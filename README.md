# ğŸï¸ Azure Databricks Formula 1 Data Engineering Project

### *Azure Data Factory â€¢ Azure Databricks â€¢ PySpark â€¢ Delta Lake â€¢ Delta Live Tables*

This repository contains two complete, production-grade data engineering pipelines built using modern lakehouse architecture:

1. **Formula 1 Medallion Architecture Pipeline (Bronze â†’ Silver â†’ Gold)**
2. **Consumer Dataset Pipeline using Delta Live Tables (DLT)**

Both pipelines follow industry-standard engineering patterns and are fully modular.

---

# ğŸ“ Repository Structure

```
F1_data_engineering/
â”‚
â”œâ”€â”€ Formula1/
â”‚   â”œâ”€â”€ demo/
â”‚   â”œâ”€â”€ includes/
â”‚   â”œâ”€â”€ ingestion/          â†’ Bronze Layer
â”‚   â”œâ”€â”€ trans/              â†’ Silver & Analytics Layer
â”‚   â”œâ”€â”€ gold/               â†’ Gold Layer (Curated)
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ dlt-yt/
â”‚   â””â”€â”€ DLT_Root/
â”‚       â”œâ”€â”€ explorations/
â”‚       â”œâ”€â”€ transformations/
â”‚       â”‚   â”œâ”€â”€ bronze/
â”‚       â”‚   â”œâ”€â”€ silver/
â”‚       â”‚   â””â”€â”€ gold/
â”‚       â””â”€â”€ utilities/
â”‚
â””â”€â”€ diagrams/               â†’ Architecture diagrams
    â”œâ”€â”€ architecture_f1.jpg
    â”œâ”€â”€ adf_pipeline_sequence.jpg
    â””â”€â”€ dlt_pipeline_dag.jpeg
```

---

# ğŸ§± 1. Formula 1 Lakehouse Pipeline (Databricks)

A full end-to-end pipeline implemented using the **Medallion Architecture**:

```
Raw Data â†’ Bronze (Raw + Incremental) â†’ Silver (Cleaned) â†’ Gold (Aggregated, BI-ready)
```

### ğŸ“Œ Architecture Diagram

![F1 Architecture](diagrams/architecture_f1.jpg)


---

# ğŸŸ¤ Bronze Layer (Raw + Incremental Ingestion)

ğŸ“‚ `Formula1/ingestion/`

Scripts include:

```
1.ingest_circuits_file.py
2.ingest_race_file.py
3.ingest_constructors_file.py
4.ingest_drivers_file.py
5.ingest_results_file.py
6.ingest_pitstops_file.py
7.ingest_lap_times_folder.py
8.ingest_qualifying_folders.py
```

### **Features**

* Loads CSV + multiline JSON into ADLS
* Converts raw files to Delta Bronze
* Adds ingestion metadata
* Supports folder ingestion
* **Incremental ingestion implemented with Delta MERGE (UPSERT) via PySpark**

---

# âšª Silver Layer (Cleaned & Standardized)

ğŸ“‚ `Formula1/trans/`

### **Key silver outputs:**

* `race_results.py`
* `driver_standings.py`
* `constructor_standings.py`
* `calculated_race_results.py`

### Silver Layer Output Table:

```
f1_presentation.race_results
```

---

# ğŸŸ¡ Gold Layer (Curated Analytics)

ğŸ“‚ `Formula1/gold/`

This layer contains **aggregated**, **business-ready**, **BI-optimized** tables.

### **Dimension Tables**

* `dim_driver`
* `dim_constructor`
* `dim_circuit`

### **Fact Tables**

* `fact_race_results`
* `fact_driver_performance`
* `fact_constructor_performance`
* `fact_season_summary`

### Purpose

âœ” Power BI dashboards
âœ” Year-over-year performance analysis
âœ” Season standings aggregation
âœ” Clean fact/dim modeling

---

# âš¡ Azure Data Factory Orchestration

ADF orchestrates the full ingestion â†’ silver â†’ gold workflow.

### ğŸ“Œ ADF Pipeline Diagram

![ADF Pipeline Sequence](diagrams/adf_pipeline_sequence.jpg)


---

# ğŸ”µ 2. Delta Live Tables Pipeline (DLT)

ğŸ“‚ `dlt-yt/DLT_Root/`

A declarative ETL pipeline using Delta Live Tables.

### **Key Features**

* Auto-managed Bronze/Silver/Gold tables
* Streaming ingestion
* Data quality expectations
* Automated lineage DAG
* Schema inference + evolution

### ğŸ“Œ DLT Pipeline Diagram

![DLT Pipeline DAG](diagrams/dlt_pipeline_dag.jpeg)


---

# ğŸ“Š DLT Outputs

### **Bronze**

* `customers_staged`
* `products_staged`
* `append_sales_staged`

### **Silver**

* Enriched customers
* Enriched products
* Enriched sales

### **Gold**

* `fact_sales`
* `business_sales` (Materialized View)

---

# ğŸš€ How to Run

## **Formula 1 Pipeline**

1. Import the `Formula1/` folder into Databricks
2. Set ADLS configs in `includes/common_functions.py`
3. Run Bronze scripts â†’ load raw into Delta
4. Run Silver scripts
5. Run Gold scripts
6. Validate tables in `f1_presentation` schema

---

## **DLT Pipeline**

1. Go to Databricks â†’ Workflows â†’ Delta Live Tables
2. Create Pipeline â†’ point to:

```
dlt-yt/DLT_Root/transformations/
```

3. Set target schema + checkpoint
4. Run pipeline
5. Explore DAG + data quality reports

---

# ğŸ§ª Technologies Used

### Cloud

* Azure Data Factory
* Azure Databricks
* ADLS Gen2

### Data Engineering

* PySpark
* Delta Lake
* Delta Live Tables
* MERGE INTO (Incremental)
* Medallion Architecture

### Analytics

* Fact/Dimension Modeling
* Aggregations & Window Functions
* KPI calculation
* BI-Ready Gold Tables

---

# ğŸ‘¨â€ğŸ’» Author

**Rehneet Singh**
